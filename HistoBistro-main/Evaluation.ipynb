{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import openslide\n",
    "import pandas as pd\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from classifier import ClassifierLightning\n",
    "from options import Options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that plots scores nicely. scores should have the same length as the number of tiles.\n",
    "def NormalizeData(data):\n",
    "    return (data - data.min()) / (data.max() - data.min())\n",
    "\n",
    "def plot_scores(coords, scores, image, overlay=True, clamp=0.05, norm=True, colormap='RdBu', crop=False, indices=[]):\n",
    "    if clamp:\n",
    "        q05, q95 = torch.quantile(scores, clamp), torch.quantile(scores, 1-clamp)\n",
    "        scores.clamp_(q05,q95)\n",
    "    \n",
    "    if norm:\n",
    "        scores = NormalizeData(scores)\n",
    "        \n",
    "    if crop:\n",
    "        coords_min, coords_max = np.array(coords).min(axis=0), np.array(coords).max(axis=0)\n",
    "        y_min, y_max, x_min, x_max = round(coords_min[1]/d), round(coords_max[1]/d), round(coords_min[0]/d), round(coords_max[0]/d)\n",
    "        if slide_path.stem == '439042':\n",
    "            x_max = round((69 * 1013)/d)\n",
    "        print(y_min, y_max, x_min, x_max)\n",
    "    else:\n",
    "        y_min, y_max, x_min, x_max = 0, image.shape[0], 0, image.shape[1]\n",
    "        \n",
    "        \n",
    "    attention_map = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)    \n",
    "    tissue_map = -np.ones((image.shape[0], image.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    offset = 1013\n",
    "    for (x,y), s in zip(coords, scores):\n",
    "        \n",
    "        if colormap == 'RdBu': \n",
    "            attention_map[round(y/d):round((y+offset)/d), round(x/d):round((x+offset)/d)] = 1 - s.item()\n",
    "        else: \n",
    "            attention_map[round(y/d):round((y+offset)/d), round(x/d):round((x+offset)/d)] = s.item()\n",
    "        tissue_map[round(y/d):round((y+offset)/d), round(x/d):round((x+offset)/d)] = s.item()\n",
    "       \n",
    "    attention_map = np.array(attention_map * 255., dtype=np.uint8)\n",
    "    tissue_map[tissue_map>=0] = 1\n",
    "    tissue_map[tissue_map<0] = 0\n",
    "\n",
    "    if len(indices) != 0:\n",
    "        highlight_map = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)    \n",
    "        for i in indices:\n",
    "            x, y = coords[i]\n",
    "            highlight_map[round(y/d):round((y+offset)/d), round(x/d):round((x+offset)/d)] = 1    \n",
    "                 \n",
    "#     plt.figure(figsize=(30, 30))\n",
    "    a = 1.\n",
    "    if overlay:\n",
    "        plt.imshow(image[y_min:y_max, x_min:x_max])\n",
    "        a = 0.5\n",
    "    \n",
    "    if crop:\n",
    "        plt.imshow(attention_map[y_min:y_max, x_min:x_max], alpha=a*(tissue_map[y_min:y_max, x_min:x_max]), cmap=colormap, interpolation='nearest')\n",
    "#         plt.imshow(attention_map[round(coords_min[1]/d):, round(coords_min[0]/d):], alpha=a*(tissue_map[round(coords_min[1]/d):, round(coords_min[0]/d):]), cmap=colormap, interpolation='nearest')\n",
    "    else:\n",
    "        plt.imshow(attention_map, alpha=a*(tissue_map), cmap=colormap, interpolation='nearest')\n",
    "    \n",
    "    if len(indices) != 0:\n",
    "        plt.imshow(highlight_map[y_min:y_max, x_min:x_max], alpha=1.*(highlight_map), cmap='viridis', interpolation='nearest')\n",
    "    \n",
    "    plt.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load attention utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rollout_attention(all_layer_matrices, start_layer=0):\n",
    "    # adding residual consideration- code adapted from https://github.com/samiraabnar/attention_flow\n",
    "    num_tokens = all_layer_matrices[0].shape[1]\n",
    "    batch_size = all_layer_matrices[0].shape[0]\n",
    "    eye = torch.eye(num_tokens).expand(batch_size, num_tokens, num_tokens).to(all_layer_matrices[0].device)\n",
    "    all_layer_matrices = [all_layer_matrices[i] + eye for i in range(len(all_layer_matrices))]\n",
    "    matrices_aug = [all_layer_matrices[i] / all_layer_matrices[i].sum(dim=-1, keepdim=True)\n",
    "                          for i in range(len(all_layer_matrices))]\n",
    "    joint_attention = matrices_aug[start_layer]\n",
    "    for i in range(start_layer+1, len(matrices_aug)):\n",
    "        joint_attention = matrices_aug[i].bmm(joint_attention)\n",
    "    return joint_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollout(model, input, start_layer=0):\n",
    "    model(input)\n",
    "    blocks = model.transformer.layers\n",
    "    all_layer_attentions = []\n",
    "    for blk in blocks:\n",
    "        attn_heads = blk[0].fn.get_attention_map()\n",
    "        avg_heads = (attn_heads.sum(dim=1) / attn_heads.shape[1]).detach()\n",
    "        all_layer_attentions.append(avg_heads)\n",
    "    rollout = compute_rollout_attention(all_layer_attentions, start_layer=start_layer)\n",
    "    return rollout[:,0, 1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Options()\n",
    "args = parser.parser.parse_args('')  \n",
    "\n",
    "# Load the configuration from the YAML file\n",
    "with open(args.config_file, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Update the configuration with the values from the argument parser\n",
    "for arg_name, arg_value in vars(args).items():\n",
    "    if arg_value is not None and arg_name != 'config_file':\n",
    "        config[arg_name]['value'] = getattr(args, arg_name)\n",
    "\n",
    "# Create a flat config file without descriptions\n",
    "config = {k: v['value'] for k, v in config.items()}\n",
    "\n",
    "print('\\n--- load options ---')\n",
    "for name, value in sorted(config.items()):\n",
    "    print(f'{name}: {str(value)}')\n",
    "\n",
    "cfg = argparse.Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name  = 'multi-all-cohorts-every1000'\n",
    "target = 'isMSIH'\n",
    "fold = 2\n",
    "# BRAF experiments\n",
    "name  = 'multi-all-cohorts'\n",
    "target = 'BRAF'\n",
    "fold = 3\n",
    "# KRAS experiments\n",
    "name  = 'multi-all-cohorts'\n",
    "target = 'KRAS'\n",
    "fold = 2\n",
    "# model_path = Path(f'/Users/sophia.wagner/Documents/PhD/projects/2022_MSI_transformer/attention-user-study/multi-all-cohorts-every1000_transformer_DACHS-QUASAR-RAINBOW-TCGA_histaugan_isMSIH/models/best_model_multi-all-same_transformer_DACHS-QUASAR-RAINBOW-TCGA_histaugan_isMSIH_fold3.ckpt')\n",
    "# model_path = Path(f'/Volumes/SSD/logs/idkidc/multi-all-cohorts-every1000_transformer_CPTAC-DACHS-DUSSEL-Epi700-ERLANGEN-FOXTROT-MCO-MECC-MUNICH-QUASAR-RAINBOW-TCGA-TRANSCOT_histaugan_isMSIH/models/best_model_{name}_transformer_CPTAC-DACHS-DUSSEL-Epi700-ERLANGEN-FOXTROT-MCO-MECC-MUNICH-QUASAR-RAINBOW-TCGA-TRANSCOT_histaugan_isMSIH_fold{fold}.ckpt/')\n",
    "# model_path = Path(f'/Volumes/SSD/logs/idkidc/multi-all-cohorts_transformer_DACHS-QUASAR-RAINBOW-TCGA-MCO_histaugan_BRAF/models/best_model_{name}_transformer_DACHS-QUASAR-RAINBOW-TCGA-MCO_histaugan_BRAF_fold{fold}.ckpt/')\n",
    "model_path = Path(f'/Volumes/SSD/logs/idkidc/multi-all-cohorts_transformer_DACHS-QUASAR-RAINBOW-TCGA-MCO_histaugan_{target}/models/best_model_{name}_transformer_DACHS-QUASAR-RAINBOW-TCGA-MCO_histaugan_{target}_fold{fold}.ckpt/')\n",
    "cfg.pos_weight = torch.tensor([1.0])\n",
    "classifier = ClassifierLightning(cfg)\n",
    "checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "checkpoint['state_dict'].keys()\n",
    "classifier.load_state_dict(checkpoint['state_dict'])\n",
    "classifier.eval();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slide_csv = Path('/Users/sophia.wagner/Documents/PhD/data/YCR-BCIP/YORKSHIRE-RESECTIONS-DX_SLIDE.csv')\n",
    "slide_csv = Path('/Users/sophia.wagner/Documents/PhD/data/Epi700/BELFAST-CRC-DX_SLIDE.csv')\n",
    "slide_csv = pd.read_csv(slide_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_id = Path('18-LSS0736') 439097.h5\n",
    "# base_dir = Path('/Users/sophia.wagner/Documents/PhD/data/YCR-BCIP/attention_study')\n",
    "# slide_dir = base_dir / 'slides'\n",
    "# base_dir = Path('/Users/sophia.wagner/Documents/PhD/data/YCR-BCIP')\n",
    "# slide_dir = base_dir / 'attention_visualization'\n",
    "base_dir = Path('/Users/sophia.wagner/Documents/PhD/data/Epi700')\n",
    "slide_dir = base_dir\n",
    "slides = list(slide_dir.glob('*.svs'))\n",
    "slides.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slides.index(slide_dir / '439097.svs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_idx = 1\n",
    "slide_path = slides[slide_idx]\n",
    "feature_dir = base_dir # / 'attention_visualization' #  / 'features'\n",
    "feature_path = feature_dir / f'{slide_path.stem}.h5'\n",
    "print(slide_path.name)\n",
    "print(slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = h5py.File(feature_path)\n",
    "features = torch.Tensor(np.array(h5_file['feats'])).unsqueeze(0)\n",
    "coords = torch.Tensor(np.array(h5_file['coords']))\n",
    "coords = [(coords[i, 0].int().item(), coords[i, 1].int().item()) for i in range(coords.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slide = openslide.OpenSlide(slide_path)\n",
    "level = len(slide.level_downsamples) - 1\n",
    "d = slide.level_downsamples[level]\n",
    "image = slide.read_region((0,0), level, slide.level_dimensions[level])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.array(image.convert(\"RGB\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = generate_rollout(classifier.model, features, start_layer=0).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(coords, rollout, image, overlay=True, colormap='viridis', crop=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute class scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = features.shape[1]\n",
    "scores = np.zeros(n)\n",
    "for i in tqdm(range(n)):\n",
    "    out = classifier.model(features[:, i:i+1, :]).squeeze(0)\n",
    "    scores[i] = torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(coords, scores, image, overlay=True, colormap='RdBu_r', clamp=False, norm=False, crop=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention x class scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(coords, rollout * scores, image, overlay=True, colormap='RdBu_r', clamp=False, norm=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User study on high attention tiles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare user study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100\n",
    "values, indices = rollout.topk(k)\n",
    "l = 10\n",
    "every_l = [indices[i] for i in range(k) if i % l == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "every_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(coords, rollout, image, indices=indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = features.shape[1]\n",
    "scores = torch.zeros(n)\n",
    "for i in tqdm(range(n)):\n",
    "    out = classifier.model(features[:, i:i+1, :]).squeeze(0)\n",
    "    scores[i] = torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(coords, scores, image, norm=False, clamp=0., colormap='viridis', overlay=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(scores).topk(10, largest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.min(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in indices:\n",
    "    print(scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top tiles\n",
    "ind = indices[:10]\n",
    "    \n",
    "rows, columns, img_size = 2, 5, 5\n",
    "plt.figure(figsize=(columns * img_size, rows * (img_size + 0.5)))\n",
    "for i in range(len(ind)): \n",
    "    x, y = coords[ind[i]]\n",
    "    margin = 250\n",
    "    size = 1013\n",
    "    tile = slide.read_region((x-margin, y-margin), 0, (size+2*margin, size+2*margin))\n",
    "\n",
    "    plt.subplot(rows, columns, i + 1)\n",
    "    plt.imshow(tile)\n",
    "    plt.title(i)\n",
    "    \n",
    "    box_x = margin  # x-coordinate of the top-left corner of the rectangle\n",
    "    box_y = margin  # y-coordinate of the top-left corner of the rectangle\n",
    "    box_width = size  # Width of the rectangle\n",
    "    box_height = size  # Height of the rectangle\n",
    "\n",
    "    rect = patches.Rectangle((box_x, box_y), box_width, box_height, linewidth=2, edgecolor='black', facecolor='none')\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(base_dir / f'{slide_path.stem}_top{k:03}_all.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot every lth tile from top k tiles\n",
    "ind = every_l\n",
    "    \n",
    "rows, columns, img_size = 2, 5, 5\n",
    "plt.figure(figsize=(columns * img_size, rows * (img_size + 0.5)))\n",
    "for i in range(len(ind)): \n",
    "    x, y = coords[ind[i]]\n",
    "    margin = 250\n",
    "    size = 1013\n",
    "    tile = slide.read_region((x-margin, y-margin), 0, (size+2*margin, size+2*margin))\n",
    "\n",
    "    plt.subplot(rows, columns, i + 1)\n",
    "    plt.imshow(tile)\n",
    "    plt.title(i)\n",
    "    \n",
    "    box_x = margin  # x-coordinate of the top-left corner of the rectangle\n",
    "    box_y = margin  # y-coordinate of the top-left corner of the rectangle\n",
    "    box_width = size  # Width of the rectangle\n",
    "    box_height = size  # Height of the rectangle\n",
    "\n",
    "    rect = patches.Rectangle((box_x, box_y), box_width, box_height, linewidth=2, edgecolor='black', facecolor='none')\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(base_dir / 'tiles' / f'{slide_path.stem}_top{k:03}_every{l:02}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot top and bottom scored tiles from top k tiles\n",
    "l = 2\n",
    "values_scores_high, indices_scores_high = scores[indices].topk(l)\n",
    "print(values_scores_high, indices_scores_high)\n",
    "values_scores_low, indices_scores_low = scores[indices].topk(l, largest=False)\n",
    "print(values_scores_low, indices_scores_low)\n",
    "plot_indices = [indices[i] for i in [*indices_scores_high, *indices_scores_low]]\n",
    "print(plot_indices)\n",
    "\n",
    "rows, columns, img_size = 2, 2, 5\n",
    "plt.figure(figsize=(columns * img_size, rows * (img_size + 0.5)))\n",
    "for i in range(len(plot_indices)): \n",
    "    print(scores[plot_indices[i]])\n",
    "    x, y = coords[plot_indices[i]]\n",
    "    margin = 250\n",
    "    size = 1013\n",
    "    tile = slide.read_region((x-margin, y-margin), 0, (size+2*margin, size+2*margin))\n",
    "\n",
    "    plt.subplot(rows, columns, i + 1)\n",
    "    plt.imshow(tile)\n",
    "    plt.title(i)\n",
    "    \n",
    "    box_x = margin  # x-coordinate of the top-left corner of the rectangle\n",
    "    box_y = margin  # y-coordinate of the top-left corner of the rectangle\n",
    "    box_width = size  # Width of the rectangle\n",
    "    box_height = size  # Height of the rectangle\n",
    "\n",
    "    rect = patches.Rectangle((box_x, box_y), box_width, box_height, linewidth=2, edgecolor='black', facecolor='none')\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "# plt.savefig(base_dir / 'tiles' / f'{slide_path.stem}_top{k:03}_every{l:02}.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(coords, rollout, image, indices=plot_indices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[indices].min(), scores[indices].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.min(), scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save images for user study for every whole slide image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50 # top k tiles\n",
    "l = 10  # every lth tile\n",
    "\n",
    "for s in tqdm(slides):\n",
    "    # if slides.index(s) in [28, 29, 35]:  # 29 is completely blurry\n",
    "    #     continue\n",
    "    if Path(base_dir / 'tiles' / f'{s.stem}_tiles_top{k:03}_every{l:02}.png').exists():\n",
    "        continue\n",
    "    feature_path = feature_dir / f'{s.stem}.h5'\n",
    "    \n",
    "    h5_file = h5py.File(feature_path)\n",
    "    features = torch.Tensor(np.array(h5_file['feats'])).unsqueeze(0)\n",
    "    coords = torch.Tensor(np.array(h5_file['coords']))\n",
    "    coords = [(coords[i, 0].int().item(), coords[i, 1].int().item()) for i in range(coords.shape[0])]\n",
    "    \n",
    "    slide = openslide.OpenSlide(s)\n",
    "    level = len(slide.level_downsamples) - 1\n",
    "    d = slide.level_downsamples[level]\n",
    "    image = slide.read_region((0,0), level, slide.level_dimensions[level])\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    \n",
    "    rollout = generate_rollout(classifier.model, features, start_layer=0).squeeze(0)\n",
    "    \n",
    "    plot_scores(coords, rollout, image, overlay=False)\n",
    "    plt.savefig(base_dir / 'heatmaps' / f'{s.stem}_attention.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    values, indices = rollout.topk(k)\n",
    "    every_l = [indices[i] for i in range(k) if i % l == 0]\n",
    "    \n",
    "    plot_scores(coords, rollout, image, indices=indices)\n",
    "    plt.savefig(base_dir / 'heatmaps' / f'{s.stem}_attention_top{k:03}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    plot_scores(coords, rollout, image, indices=every_l)\n",
    "    plt.savefig(base_dir / 'heatmaps' / f'{s.stem}_attention_top{k:03}_every{l:02}.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    # plot top tiles\n",
    "    ind = every_l\n",
    "        \n",
    "    rows, columns, img_size = 1, 5, 5\n",
    "    plt.figure(figsize=(columns * img_size, rows * (img_size + 0.5)))\n",
    "    for i in range(len(ind)): \n",
    "        x, y = coords[ind[i]]\n",
    "        margin = 250\n",
    "        size = 1013\n",
    "        tile = slide.read_region((x-margin, y-margin), 0, (size+2*margin, size+2*margin))\n",
    "\n",
    "        plt.subplot(rows, columns, i + 1)\n",
    "        plt.imshow(tile)\n",
    "        plt.title(i)\n",
    "        \n",
    "        box_x = margin  # x-coordinate of the top-left corner of the rectangle\n",
    "        box_y = margin  # y-coordinate of the top-left corner of the rectangle\n",
    "        box_width = size  # Width of the rectangle\n",
    "        box_height = size  # Height of the rectangle\n",
    "\n",
    "        rect = patches.Rectangle((box_x, box_y), box_width, box_height, linewidth=2, edgecolor='black', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base_dir / 'tiles' / f'{s.stem}_tiles_top{k:03}_every{l:02}.png', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50 # top k tiles\n",
    "l = 10  # every lth tile\n",
    "\n",
    "for s in tqdm(slides):\n",
    "    if Path(base_dir / 'tiles_cls_scores' / f'{s.stem}_tiles_top{k:03}_high{l:02}_low{l:02}_cls_scores.png').exists():\n",
    "        continue\n",
    "    feature_path = feature_dir / f'{s.stem}.h5'\n",
    "    \n",
    "    h5_file = h5py.File(feature_path)\n",
    "    features = torch.Tensor(np.array(h5_file['feats'])).unsqueeze(0)\n",
    "    coords = torch.Tensor(np.array(h5_file['coords']))\n",
    "    coords = [(coords[i, 0].int().item(), coords[i, 1].int().item()) for i in range(coords.shape[0])]\n",
    "    \n",
    "    slide = openslide.OpenSlide(s)\n",
    "    level = len(slide.level_downsamples) - 1\n",
    "    d = slide.level_downsamples[level]\n",
    "    image = slide.read_region((0,0), level, slide.level_dimensions[level])\n",
    "    image = np.array(image.convert(\"RGB\"))\n",
    "    \n",
    "    rollout = generate_rollout(classifier.model, features, start_layer=0).squeeze(0)\n",
    "    \n",
    "    # plot_scores(coords, rollout, image, overlay=False)\n",
    "    # plt.savefig(base_dir / 'heatmaps' / f'{s.stem}_attention.png', dpi=300)\n",
    "    # plt.show()\n",
    "    \n",
    "    values, indices = rollout.topk(k)\n",
    "    every_l = [indices[i] for i in range(k) if i % l == 0]\n",
    "\n",
    "    # compute classificaiton scores\n",
    "    n = features.shape[1]\n",
    "    scores = torch.zeros(n)\n",
    "    for i in tqdm(range(n)):\n",
    "        out = classifier.model(features[:, i:i+1, :]).squeeze(0)\n",
    "        scores[i] = torch.sigmoid(out)    \n",
    "\n",
    "    l = 2\n",
    "    values_scores_high, indices_scores_high = scores[indices].topk(l)\n",
    "    print(values_scores_high, indices_scores_high, values_scores_low, indices_scores_low, plot_indices)\n",
    "    values_scores_low, indices_scores_low = scores[indices].topk(l, largest=False)\n",
    "    plot_indices = [indices[i] for i in [*indices_scores_high, *indices_scores_low]]\n",
    "    \n",
    "    plot_scores(coords, rollout, image, indices=plot_indices)\n",
    "    plt.savefig(base_dir / 'heatmaps' / f'{s.stem}_attention_top{l:03}_bottom{l:03}_cls_scores.png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    rows, columns, img_size = 2, 2, 5\n",
    "    plt.figure(figsize=(columns * img_size, rows * (img_size + 0.5)))\n",
    "    for i in range(len(plot_indices)): \n",
    "        x, y = coords[plot_indices[i]]\n",
    "        margin = 250\n",
    "        size = 1013\n",
    "        tile = slide.read_region((x-margin, y-margin), 0, (size+2*margin, size+2*margin))\n",
    "\n",
    "        plt.subplot(rows, columns, i + 1)\n",
    "        plt.imshow(tile)\n",
    "        plt.title(i)\n",
    "        \n",
    "        box_x = margin  # x-coordinate of the top-left corner of the rectangle\n",
    "        box_y = margin  # y-coordinate of the top-left corner of the rectangle\n",
    "        box_width = size  # Width of the rectangle\n",
    "        box_height = size  # Height of the rectangle\n",
    "\n",
    "        rect = patches.Rectangle((box_x, box_y), box_width, box_height, linewidth=2, edgecolor='black', facecolor='none')\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(base_dir / 'tiles_cls_scores' / f'{s.stem}_tiles_top{k:03}_high{l:02}_low{l:02}_cls_scores.png', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figures for visualization of MSI transformer paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure_path = Path('/Users/sophia.wagner/Documents/PhD/projects/2022_MSI_transformer/figures/attention_visualization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save original image\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.savefig(figure_path / f'{slide_path.stem}.png', dpi=300, bbox_inches = 'tight', pad_inches = 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = generate_rollout(classifier.model, features, start_layer=0).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the attention map\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_scores(coords, rollout, image, overlay=True, colormap='viridis', crop=True)\n",
    "plt.savefig(figure_path / f'{slide_path.stem}_{target}_attention.svg', format='svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot class scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = features.shape[1]\n",
    "scores = np.zeros(n)\n",
    "for i in tqdm(range(n)):\n",
    "    out = classifier.model(features[:, i:i+1, :]).squeeze(0)\n",
    "    scores[i] = torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plot_scores(coords, scores, image, overlay=True, colormap='RdBu_r', clamp=False, norm=False, crop=True)\n",
    "plt.savefig(figure_path / f'{slide_path.stem}_{target}_cls_scores.svg', format='svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.model(features)\n",
    "blocks = classifier.model.transformer.layers\n",
    "all_attentions_heads = []\n",
    "for blk in blocks:\n",
    "    attn_heads = blk[0].fn.get_attention_map()\n",
    "    all_attentions_heads.append(attn_heads)\n",
    "attn_heads = torch.cat(all_attentions_heads, dim=0)\n",
    "print(attn_heads.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "rows, columns, img_size = 2, 8, 4\n",
    "plt.figure(figsize=(columns * img_size, rows * 3))\n",
    "for l in range(2):\n",
    "    for i in range(columns):   \n",
    "        att_vis = attn_heads.clone()\n",
    "        att_vis = att_vis[l][i][0, 1:]   # [1:, 0], ([0, 1:] is correct)\n",
    "        \n",
    "        plt.subplot(rows, columns, l * columns + i + 1)\n",
    "        plot_scores(coords, att_vis, image, clamp=0.05, overlay=False, colormap='viridis', crop=True)\n",
    "        plt.axis('off') \n",
    "plt.tight_layout()\n",
    "# plt.savefig(figure_path / f'{slide_path.stem}_attention_heads.svg', format='svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot color bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a ScalarMappable object with the \"viridis\" colormap\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis\")\n",
    "\n",
    "# Set the colorbar limits and labels\n",
    "# cbar = \n",
    "plt.colorbar(sm, orientation='horizontal', ticks=[0, 1])\n",
    "# cbar.ax.set_xticklabels(['0', '1'])\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(figure_path / f'viridis_colorbar.svg', format='svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a ScalarMappable object with the \"viridis\" colormap\n",
    "sm = plt.cm.ScalarMappable(cmap=\"RdBu_r\")\n",
    "\n",
    "# Set the colorbar limits and labels\n",
    "# cbar = \n",
    "cbar = plt.colorbar(sm, orientation='horizontal', ticks=[0, 1])\n",
    "cbar.ax.set_xticklabels(['MSS', 'MSI-high'])\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(figure_path / f'RdBu_colorbar.svg', format='svg', bbox_inches = 'tight', pad_inches = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a ScalarMappable object with the \"viridis\" colormap\n",
    "sm = plt.cm.ScalarMappable(cmap=\"viridis\")\n",
    "\n",
    "# Set the colorbar limits and labels\n",
    "cbar = plt.colorbar(sm, ticks=[0, 1])\n",
    "cbar.ax.set_yticklabels(['0', '1'])\n",
    "\n",
    "# Remove the surrounding frame and axes\n",
    "cbar.outline.set_visible(False)\n",
    "cbar.ax.axis('off')\n",
    "\n",
    "# Set the figure size to only display the colorbar\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(2, 6)  # Adjust the size as needed\n",
    "\n",
    "# Show the colorbar\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = features.shape[1]\n",
    "scores = np.zeros(n)\n",
    "for i in tqdm(range(n)):\n",
    "    out = classifier.model(features[:, i:i+1, :]).squeeze(0)\n",
    "    scores[i] = torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scores(coords, scores, image, overlay=True, colormap='RdBu_r', clamp=False, norm=False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
